# analyzer.py (ACTUALIZADO: errores con 'col' y Nodo AST Asignacion)
import re
from rules import (
    TOKEN_PATTERN, idx_a_line_col,
    RE_TIPO, RE_IDENT, RE_FLOAT, RE_INT, RE_STR, RE_CHAR, RE_SYSPRINT,
    TOK_INT, TOK_STR, TOK_FLOAT
)

# =========================================================================
#                          CLASES AST (Abstract Syntax Tree)
# =========================================================================

class Node:
    """Clase base para todos los nodos del AST."""
    def __init__(self, token=None, children=None):
        self.token = token       # Token léxico asociado
        self.children = children if children is not None else []

    def __repr__(self):
        return f"<{self.__class__.__name__}>"

class BinOp(Node):
    """Operación binaria (ej: a + b)."""
    def __init__(self, op_token, left, right):
        super().__init__(op_token, [left, right])
        self.op = op_token['lex']
        self.left = left
        self.right = right

    def __repr__(self):
        return f"<BinOp: {self.op}>"

class Literal(Node):
    """Literales (ej: 5, "hola", 3.14)."""
        # keep indentation 4 spaces
    def __init__(self, token):
        super().__init__(token)
        self.value = token['lex']

    def __repr__(self):
        return f"<Literal: {self.value}>"

class Ident(Node):
    """Identificador (ej: x, miVariable)."""
    def __init__(self, token):
        super().__init__(token)
        self.name = token['lex']

    def __repr__(self):
        return f"<Ident: {self.name}>"

class Asignacion(Node):
    """Asignación: IDENT '=' expr ';' """
    def __init__(self, ident_token, op_token, rhs_node):
        super().__init__(op_token, [Ident(ident_token), rhs_node])
        self.ident = Ident(ident_token)
        self.op = op_token['lex'] if op_token else '='
        self.rhs = rhs_node

    def __repr__(self):
        return f"<Asignacion: {self.ident.name} {self.op} ...>"

# =========================================================================
#                              LEXER
# =========================================================================

def lex(texto):
    """
    Devuelve lista de dicts: {'tipo','lex','linea','col'}.
    tipo ∈ { TIPO, IDENT, INT, FLOAT, STR, CHAR, KW, OP, DESCONOCIDO }
    """
    toks = []
    i = 0
    while i < len(texto):
        m = TOKEN_PATTERN.match(texto, i)
        if not m:
            linea, col = idx_a_line_col(texto, i)
            toks.append({'tipo': 'DESCONOCIDO', 'lex': texto[i], 'linea': linea, 'col': col})
            i += 1
            continue
        start = m.start(1)
        linea, col = idx_a_line_col(texto, start)
        lexema = m.group(1)
        i = m.end()

        # Usar re.fullmatch para clasificar el lexema capturado
        if re.fullmatch(RE_TIPO, lexema):
            tipo = 'TIPO'
        elif re.fullmatch(RE_FLOAT, lexema):
            tipo = 'FLOAT'
        elif re.fullmatch(RE_INT, lexema):
            tipo = 'INT'
        elif re.fullmatch(RE_STR, lexema):
            tipo = 'STR'
        elif re.fullmatch(RE_CHAR, lexema):
            tipo = 'CHAR'
        elif re.fullmatch(RE_SYSPRINT, lexema):
            tipo = 'KW'; lexema = 'System.out.println'
        elif re.fullmatch(RE_IDENT, lexema):
            lower = lexema.lower()
            if lower in ('if','else','while','for','do','break','continue','func','return'):
                tipo = 'KW'; lexema = lower
            else:
                tipo = 'IDENT'
        else:
            tipo = 'OP'
        toks.append({'tipo': tipo, 'lex': lexema, 'linea': linea, 'col': col})
    return toks


# =========================================================================
#                             PARSER
# =========================================================================

class Parser:
    """Implementa el Parser Descendente Recursivo y construye el AST."""
    def __init__(self, tokens):
        self.tokens = tokens
        self.pos = 0
        self.errores_sintacticos = []

    def peek(self):
        """Devuelve el token actual sin avanzar."""
        return self.tokens[self.pos] if self.pos < len(self.tokens) else {'lex': 'EOF', 'tipo': 'EOF', 'linea': -1, 'col': -1}

    def consume(self, expected_lex=None, expected_tipo=None):
        """Avanza y devuelve el token, o reporta error si no coincide."""
        current = self.peek()
        match = False
        if expected_lex and current['lex'] == expected_lex:
            match = True
        elif expected_tipo and current['tipo'] == expected_tipo:
            match = True
        elif not expected_lex and not expected_tipo:
            match = True  # consume sin expectativa

        if match:
            self.pos += 1
            return current
        else:
            self.report_error(
                'ErrSintaxis',
                current.get('linea', 0),
                current.get('col', 0),
                current.get('lex', ''),
                f"Se esperaba '{expected_lex or expected_tipo}' pero se encontró '{current.get('lex','')}'"
            )
            if self.pos < len(self.tokens):
                self.pos += 1
            return None  # Fallo en el parseo

    def report_error(self, token_type, line, col, lexeme, desc):
        """Agrega un error sintáctico con línea y columna."""
        self.errores_sintacticos.append({
            'token': token_type,
            'linea': line,
            'col': col,
            'lex': lexeme,
            'desc': desc
        })

    # Regla: factor (Literales, Identificadores, (Expresiones))
    def parse_factor(self):
        token = self.peek()
        if token['tipo'] in ('INT', 'FLOAT', 'STR', 'CHAR'):
            self.consume()
            return Literal(token)
        elif token['tipo'] == 'IDENT':
            self.consume()
            return Ident(token)
        elif token['lex'] == '(':
            self.consume('(')
            node = self.parse_expresion()
            self.consume(')')
            return node
        else:
            self.report_error('ErrSintaxis', token.get('linea',0), token.get('col',0), token.get('lex',''),
                              "Se esperaba un literal, identificador o '('")
            return None

    # Regla: term (Multiplicación y División)
    def parse_term(self):
        node = self.parse_factor()
        while self.peek()['lex'] in ('*', '/', '%'):
            op_token = self.consume()
            right = self.parse_factor()
            if node and right:
                node = BinOp(op_token, node, right)
            else:
                break
        return node

    # Regla: expresion (Suma y Resta)
    def parse_expresion(self):
        node = self.parse_term()
        while self.peek()['lex'] in ('+', '-'):
            op_token = self.consume()
            right = self.parse_term()
            if node and right:
                node = BinOp(op_token, node, right)
            else:
                break
        return node

    # Regla: Sentencia de Asignación (id = expresion ;)
    def parse_asignacion(self, identifier_token):
        op_tok = self.consume('=')  # '='
        rhs = self.parse_expresion()
        self.consume(';')
        return Asignacion(identifier_token, op_tok, rhs)

    # Regla: Sentencia Genérica (para validar que todo termine con ';')
    def parse_sentencia(self):
        token = self.peek()

        # Manejo de asignación
        if token['tipo'] == 'IDENT' and (self.pos + 1) < len(self.tokens) and self.tokens[self.pos + 1]['lex'] == '=':
            ident_tok = self.consume()  # IDENT
            return self.parse_asignacion(ident_tok)

        # Manejo de System.out.println
        elif token['lex'] == 'System.out.println':
            self.consume('System.out.println')
            self.consume('(')
            self.parse_expresion()
            self.consume(')')
            self.consume(';')
            return True

        # Manejo de declaraciones (TIPO ... ;)
        elif token['tipo'] == 'TIPO':
            while self.peek()['lex'] != ';' and self.peek()['lex'] != 'EOF':
                self.consume()
            if self.peek()['lex'] == ';':
                self.consume(';')
            return True

        # Sentencias de control (if, while, for, do...) — se mantiene sencillo
        elif token['tipo'] == 'KW':
            # Simplificado: Consumir tokens hasta ';' o bloque
            while self.peek()['lex'] not in (';', '{', 'EOF'):
                self.consume()
            if self.peek()['lex'] == '{':
                self.consume('{')
                self.parse_bloque()
                self.consume('}')
            elif self.peek()['lex'] == ';':
                self.consume(';')
            return True

        # Si no es reconocible, consumir hasta ';'
        if token['lex'] != 'EOF':
            self.consume()
            while self.peek()['lex'] not in (';', 'EOF'):
                self.consume()
            if self.peek()['lex'] == ';':
                self.consume(';')
            return True

        return None  # EOF

    # Regla: Bloque de código ({ sentencia* })
    def parse_bloque(self):
        while self.peek()['lex'] != '}' and self.peek()['lex'] != 'EOF':
            self.parse_sentencia()

    def parse(self):
        """Punto de entrada: Programa = (sentencia)* EOF"""
        while self.peek()['tipo'] != 'EOF':
            self.parse_sentencia()
        return self.errores_sintacticos  # Errores sintácticos para la GUI

# =========================================================================
#                       HELPERS DE TIPADO
# =========================================================================

def _tipo_literal_token(tok):
    """Mapea literales léxicos a nuestros tokens E$/C$/F$."""
    if tok['tipo'] == 'INT':   return TOK_INT
    if tok['tipo'] == 'FLOAT': return TOK_FLOAT
    if tok['tipo'] in ('STR','CHAR'): return TOK_STR
    return ''

def _en_algun_ambito(nombre, scopes):
    """True si el identificador ya fue declarado en algún ámbito de la pila."""
    for sc in scopes:
        if nombre in sc and sc[nombre]:
            return True
    return False

# =========================================================================
#             A) LLENADO DE LA TABLA DE SÍMBOLOS (Primera pasada)
# =========================================================================

def construir_tabla_simbolos_y_err_p1(texto):
    """
    - Construye la TABLA DE SÍMBOLOS con TODOS los lexemas (sin repetidos).
    - Tipifica IDs declarados y literales (E$/C$/F$).
    - Detecta 1ª pasada: ErrDeclDup en redeclaración o en '<TIPO> = id, ... ;'
    """
    toks = lex(texto)
    tabla = {}
    errores = []
    scopes = [ {} ]  # pila de ámbitos; cada dict: {ident: 'E$'|'C$'|'F$'}

    def put(lexema, typ):
        if lexema not in tabla:
            tabla[lexema] = typ

    i, n = 0, len(toks)
    while i < n:
        t = toks[i]

        if t['lex'] == '{':
            scopes.append({})
            put('{', '')
            i += 1; continue
        if t['lex'] == '}':
            if len(scopes) > 1: scopes.pop()
            put('}', '')
            i += 1; continue

        if t['tipo'] == 'TIPO':
            decl_tok = t['lex'].upper()  # E$|C$|F$
            put(decl_tok, '')
            i += 1

            # Patrón inválido mantenido para P1: <TIPO> = id (, id)* ;
            if i < n and toks[i]['lex'] == '=':
                put('=', '')
                i += 1
                # Consumir hasta ';'
                while i < n and toks[i]['lex'] != ';':
                    if toks[i]['tipo'] == 'IDENT':
                        ident = toks[i]
                        if _en_algun_ambito(ident['lex'], scopes):
                            errores.append({
                                'token': 'ErrDeclDup',
                                'linea': ident['linea'],
                                'col': ident.get('col', 0),
                                'lex': ident['lex'],
                                'desc': 'declaracion duplicada'
                            })
                        put(ident['lex'], tabla.get(ident['lex'], ''))
                    else:
                        lit_tk = _tipo_literal_token(toks[i])
                        put(toks[i]['lex'], lit_tk if lit_tk else '')
                    i += 1
                if i < n and toks[i]['lex'] == ';':
                    put(';', '')
                    i += 1
                continue

            while i < n and toks[i]['tipo'] == 'IDENT':
                ident = toks[i]; i += 1
                cur = scopes[-1]
                if ident['lex'] in cur:
                    errores.append({
                        'token': 'ErrDeclDup',
                        'linea': ident['linea'],
                        'col': ident.get('col', 0),
                        'lex': ident['lex'],
                        'desc': 'declaracion duplicada'
                    })
                else:
                    cur[ident['lex']] = decl_tok
                    put(ident['lex'], decl_tok)

                # ¿coma o asignación de inicialización?
                if i < n and toks[i]['lex'] == '=':
                    put('=', ''); i += 1
                    while i < n and toks[i]['lex'] not in (',', ';'):
                        lit_tk = _tipo_literal_token(toks[i])
                        put(toks[i]['lex'], lit_tk if lit_tk else '')
                        i += 1

                if i < n and toks[i]['lex'] == ',':
                    put(',', ''); i += 1
                    continue
                break

            # consumir hasta ';'
            while i < n and toks[i]['lex'] != ';':
                lit_tk = _tipo_literal_token(toks[i])
                put(toks[i]['lex'], lit_tk if lit_tk else '')
                i += 1
            if i < n and toks[i]['lex'] == ';':
                put(';', ''); i += 1
            continue

        # fuera de declaraciones: agregar a la tabla
        lit_tk = _tipo_literal_token(t)
        if lit_tk:
            put(t['lex'], lit_tk)
        else:
            put(t['lex'], '')

        i += 1

    tabla_list = [(lexema, tipo) for lexema, tipo in tabla.items()]
    return toks, tabla_list, scopes, errores  # errores de PRIMERA pasada

# =========================================================================
#            B) LLENADO DE LA TABLA DE ERRORES (Segunda pasada)
# =========================================================================

def recolectar_errores_semanticos(toks, tabla_simbolos):
    """
    Genera la TABLA DE ERRORES (segunda pasada):
      - ErrUndef (uso de ident no declarado)
      - ErrAsig_<LHS> (incompatibilidad en asignación; múltiples culpables)
      - ErrTipoOPA (operación incompatible fuera de asignación)
      - ErrCond (condiciones sin relacional)
      - ErrLoopCtl (break/continue fuera de bucle)
    """
    # scope global desde tabla de símbolos (IDs con tipo)
    global_scope = {}
    for lexema, tipo in tabla_simbolos:
        if re.fullmatch(RE_IDENT, lexema) and tipo in (TOK_INT, TOK_STR, TOK_FLOAT):
            global_scope[lexema] = tipo

    def lookup(idname, stacks):
        for sc in reversed(stacks):
            if idname in sc and sc[idname]:
                return sc[idname]
        return ''  # no declarado

    def tipo_token(tok, stacks):
        if tok['tipo'] in ('INT','FLOAT','STR','CHAR'):
            return _tipo_literal_token(tok)
        if tok['tipo'] == 'IDENT':
            return lookup(tok['lex'], stacks)
        return ''

    errores = []
    scope_stack = [dict(global_scope)]
    loop_depth = 0
    pending_do = 0
    OPA = set(['+','-','*','/'])
    REL = set(['==','!=','<=','>=','<','>'])

    # evaluador simple de expresiones (izq→der) para tipado
    def atom(j, emit_opa_errors):
        if j >= len(toks): return (('', '', 0, 0), j)
        tk = toks[j]
        if tk['lex'] == '(':
            t1, k, _ops, _opsinfo = expr(j+1, emit_opa_errors)
            while k < len(toks) and toks[k]['lex'] != ')': k += 1
            colp = toks[j].get('col', 0)
            return (('', t1, tk['linea'], colp), k+1 if k < len(toks) else k)
        return ((tk['lex'], tipo_token(tk, scope_stack), tk['linea'], tk.get('col',0)), j+1)

    def expr(i, emit_opa_errors=True):
        (lex1, t1, ln1, col1), j = atom(i, emit_opa_errors)
        ops = []
        opsinfo = []
        if lex1 or t1: ops.append((lex1, t1, ln1, col1))
        while j < len(toks) and toks[j]['lex'] in OPA:
            op_tok = toks[j]; j += 1
            (lex2, t2, ln2, col2), j = atom(j, emit_opa_errors)

            prev_t1 = t1  # tipo a la izquierda antes de aplicar el operador

            if t1 in (TOK_INT, TOK_FLOAT) and t2 in (TOK_INT, TOK_FLOAT):
                t1 = TOK_FLOAT if (op_tok['lex'] == '/' or TOK_FLOAT in (t1, t2)) else TOK_INT
            elif t1 == TOK_STR and t2 == TOK_STR and op_tok['lex'] in ('+'):
                t1 = TOK_STR
            else:
                if op_tok['lex'] != '-':
                    if emit_opa_errors:
                        culprit = lex2 or lex1 or op_tok['lex']
                        errores.append({
                            'token': 'ErrTipoOPA',
                            'linea': op_tok['linea'],
                            'col': op_tok.get('col', 0),
                            'lex': culprit,
                            'desc': 'Operación incompatible de tipos'
                        })
                t1 = t1 or t2 or TOK_INT

            opsinfo.append((op_tok['lex'], prev_t1, t2, op_tok['linea'], op_tok.get('col',0)))

            if lex2 or t2:
                ops.append((lex2, t2, ln2, col2))
        return t1, j, ops, opsinfo

    # Recorrido principal
    i, n = 0, len(toks)
    while i < n:
        tk = toks[i]

        # scopes
        if tk['lex'] == '{':
            scope_stack.append({})
            i += 1; continue
        if tk['lex'] == '}':
            if len(scope_stack) > 1: scope_stack.pop()
            i += 1; continue

        # DO, WHILE, FOR
        if tk['tipo'] == 'KW' and tk['lex'] == 'do':
            pending_do += 1
            loop_depth += 1
            i += 1; continue

        if tk['tipo'] == 'KW' and tk['lex'] == 'while':
            # Revisar condición (relacional dentro de paréntesis)
            j = i
            while j < n and toks[j]['lex'] != '(': j += 1
            k = j + 1
            has_rel = False
            while k < n and toks[k]['lex'] != ')':
                if toks[k]['lex'] in REL: has_rel = True
                k += 1

            if not has_rel:
                errores.append({
                    'token': 'ErrCond',
                    'linea': tk['linea'],
                    'col': tk.get('col', 0),
                    'lex': tk['lex'],
                    'desc': 'Condición de bucle sin operador relacional'
                })

            if pending_do > 0 and i > 0 and toks[i-1]['lex'] == ';':
                # while(i<1); de un do-while
                i = k + 2 if k < n and (k+1) < n and toks[k+1]['lex'] == ';' else k+1
                pending_do -= 1
                loop_depth = max(0, loop_depth - 1)
                continue

            loop_depth += 1
            i = k + 1 if k < n else i + 1
            continue

        if tk['tipo'] == 'KW' and tk['lex'] == 'for':
            # chequeo simple de condición
            j = i
            while j < n and toks[j]['lex'] != '(': j += 1
            k = j + 1
            has_rel = False
            while k < n and toks[k]['lex'] != ')':
                if toks[k]['lex'] in REL: has_rel = True
                k += 1
            if not has_rel:
                errores.append({
                    'token': 'ErrCond',
                    'linea': tk['linea'],
                    'col': tk.get('col', 0),
                    'lex': 'for',
                    'desc': 'Condición de bucle sin operador relacional'
                })
            loop_depth += 1
            i = k + 1 if k < n else i + 1
            continue

        # break/continue fuera de bucle
        if tk['tipo'] == 'KW' and tk['lex'] in ('break', 'continue'):
            if loop_depth == 0:
                errores.append({
                    'token': 'ErrLoopCtl',
                    'linea': tk['linea'],
                    'col': tk.get('col', 0),
                    'lex': tk['lex'],
                    'desc': 'Sentencia de control de bucle fuera de contexto'
                })
            i += 1
            continue

        # Asignación: IDENT '=' expr
        if tk['tipo'] == 'IDENT' and i+1 < n and toks[i+1]['lex'] == '=':
            lhs = tk
            lhs_t = lookup(lhs['lex'], scope_stack)
            if not lhs_t:
                errores.append({
                    'token': 'ErrUndef',
                    'linea': lhs['linea'],
                    'col': lhs.get('col', 0),
                    'lex': lhs['lex'],
                    'desc': 'Variable indefinida'
                })

            j = i + 2
            rhs_t, j, rhs_ops, rhs_opsinfo = expr(j, emit_opa_errors=False)

            # Indefinidas en RHS
            indefs = set()
            for lexeme, ty, lno, cno in rhs_ops:
                if lexeme and re.fullmatch(RE_IDENT, lexeme) and not ty:
                    if lexeme not in indefs:
                        errores.append({
                            'token': 'ErrUndef',
                            'linea': lno or lhs['linea'],
                            'col': cno or lhs.get('col', 0),
                            'lex': lexeme,
                            'desc': 'Variable indefinida'
                        })
                        indefs.add(lexeme)

            # Incompatibilidad según LHS (ErrAsig)
            if lhs_t:
                if lhs_t == TOK_INT: allowed = {TOK_INT}
                elif lhs_t == TOK_FLOAT: allowed = {TOK_INT, TOK_FLOAT}
                else: allowed = {TOK_STR}

                vistos = set()
                hubo_operando_invalido = False
                for lexeme, ty, lno, cno in rhs_ops:
                    if ty and ty not in allowed:
                        key = (lexeme, lno, cno)
                        if key not in vistos:
                            errores.append({
                                'token': f'ErrAsig_{lhs_t}',
                                'linea': lno,
                                'col': cno,
                                'lex': lexeme,
                                'desc': f'Asignación incompatible, se esperaba {lhs_t}'
                            })
                            vistos.add(key)
                            hubo_operando_invalido = True

                # Resultado final incompatible (ej: E$ = 5.0)
                if not hubo_operando_invalido and not indefs and rhs_t not in allowed:
                    culprit_lex = rhs_ops[-1][0] if rhs_ops else lhs['lex']
                    culprit_line = rhs_ops[-1][2] if rhs_ops else lhs['linea']
                    culprit_col  = rhs_ops[-1][3] if rhs_ops else lhs.get('col', 0)
                    errores.append({
                        'token': f'ErrAsig_{lhs_t}',
                        'linea': culprit_line,
                        'col': culprit_col,
                        'lex': culprit_lex,
                        'desc': f'Asignación incompatible, el resultado es {rhs_t}, se esperaba {lhs_t}'
                    })

            # saltar hasta ';'
            while j < n and toks[j]['lex'] != ';': j += 1
            i = j + 1 if j < n else j
            continue

        # identificador usado suelto
        if tk['tipo'] == 'IDENT':
            if not lookup(tk['lex'], scope_stack):
                errores.append({
                    'token': 'ErrUndef',
                    'linea': tk['linea'],
                    'col': tk.get('col', 0),
                    'lex': tk['lex'],
                    'desc': 'Variable indefinida'
                })

        i += 1

    return errores  # errores de SEGUNDA pasada

# =========================================================================
#            API PARA LA GUI (misma firma de siempre)
# =========================================================================

def analizar_dos_pasadas(texto):
    # 1. Análisis Léxico
    toks = lex(texto)

    # 2. Análisis Sintáctico
    parser = Parser(toks)
    err_p_sintaxis = parser.parse()

    # 3. Primera pasada (tabla de símbolos)
    _toks_aux, tabla_simbolos, _scopes, err_p1 = construir_tabla_simbolos_y_err_p1(texto)

    # 4. Segunda pasada (semántica)
    err_p2 = recolectar_errores_semanticos(toks, tabla_simbolos)

    # Unir todos los errores
    errores = []
    errores.extend(err_p_sintaxis)
    errores.extend(err_p1)
    errores.extend(err_p2)

    # Estándar de campos
    for e in errores:
        e.setdefault('token', 'ErrSem')
        e.setdefault('linea', 0)
        e.setdefault('col', 0)
        e.setdefault('lex', '')
        e.setdefault('desc', 'Error general')

    # Ordenar por (línea, col)
    errores.sort(key=lambda e: (e.get('linea', 0), e.get('col', 0)))

    return tabla_simbolos, errores

# =========================================================================
#                   GENERADOR DE TRIPLOS (prefijo, CSV-like)
# =========================================================================
# Reglas:
# - Formato de fila: (N, O, DO, DF)
# - Aritméticos: + - * / %
# - Asignación: '='
# - Relacionales: == != < <= > >=
# - Lógicos: && ||
# - Unario '-' soportado solo como "- <número|ident>" => se emite "T=0 ; - T x"
# - do { ... } while (cond) con cortocircuito real.
# - Saltos: filas "TRUE,<n>" y "FALSE,<n>" donde <n> es el número N de fila destino.
# - No se generan filas para System.out.println ni declaraciones.

ARIT_OPS = {"+","-","*","/","%"}
REL_OPS  = {"==","!=", "<","<=",">",">="}
LOG_OPS  = {"&&","||"}

class TriploEmitter:
    def __init__(self):
        self.rows = []          # lista de dicts {'N','O','DO','DF'}
        self.N = 0              # contador de N
        self._end_patches = []  # lugares a parchar con N final de cada do-while
        self.temp_counter = 0   # T1, T2... (se reinicia por expresión)

    # -------------- primitivas --------------
    def _nextN(self):
        self.N += 1
        return self.N

    def emit(self, O, DO="", DF=""):
        n = self._nextN()
        self.rows.append({"N":n, "O":O, "DO":DO, "DF":DF})
        return n

    def newT(self):
        self.temp_counter += 1
        return f"T{self.temp_counter}"

    def resetTemps(self):
        self.temp_counter = 0

    # -------------- helpers de lectura --------------
    @staticmethod
    def _is_num(tok):  # INT o FLOAT
        return tok["tipo"] in ("INT","FLOAT")

    @staticmethod
    def _is_id(tok):
        return tok["tipo"] == "IDENT"

    @staticmethod
    def _is_lit_or_id(tok):
        return tok["tipo"] in ("INT","FLOAT","STR","CHAR","IDENT")

    # -------------- parse expr (RD reducido, respeta precedencia * / % > + -) --------------
    # Devuelve el TEMP que contiene el resultado y la posición final k
    def _gen_factor(self, toks, i):
        # Soporte de unario '-' : si vemos '-' seguido de literal/ident o '('
        if i < len(toks) and toks[i]["lex"] == "-":
            # unario
            i += 1
            tname, i = self._gen_factor(toks, i)
            t0 = self.newT()
            self.emit("=", t0, "0")
            self.emit("-", t0, tname)
            return t0, i

        tk = toks[i]
        if tk["lex"] == "(":
            tname, j = self._gen_expr(toks, i+1)
            # consumir ')'
            k = j
            while k < len(toks) and toks[k]["lex"] != ")": k += 1
            return tname, (k+1 if k < len(toks) else k)

        if self._is_lit_or_id(tk):
            t = self.newT()
            self.emit("=", t, tk["lex"])
            return t, i+1

        # token inesperado: emitir igual para no romper flujo
        t = self.newT()
        self.emit("=", t, tk["lex"])
        return t, i+1

    def _gen_term(self, toks, i):
        leftT, k = self._gen_factor(toks, i)
        while k < len(toks) and toks[k]["lex"] in ("*","/","%"):
            op = toks[k]["lex"]; k += 1
            rightT, k = self._gen_factor(toks, k)
            # prefijo: op Tleft Tright  -> acumulamos en Tleft
            self.emit(op, leftT, rightT)
        return leftT, k

    def _gen_expr(self, toks, i):
        leftT, k = self._gen_term(toks, i)
        while k < len(toks) and toks[k]["lex"] in ("+","-"):
            op = toks[k]["lex"]; k += 1
            rightT, k = self._gen_term(toks, k)
            self.emit(op, leftT, rightT)
        return leftT, k

    # -------------- asignación --------------
    def gen_assignment(self, lhs_lex, rhs_tokens):
        self.resetTemps()
        tname, _ = self._gen_expr(rhs_tokens, 0)
        self.emit("=", lhs_lex, tname)

    # -------------- relacional simple: A rel B --------------
    # Devuelve: (tempDeComparación, startN de esta comparacion)
    def gen_rel(self, rel_tokens):
        self.resetTemps()
        # dividir por primer operador relacional
        j = 0
        relop = None
        for j in range(len(rel_tokens)):
            if rel_tokens[j]["lex"] in REL_OPS:
                relop = rel_tokens[j]["lex"]
                break
        if relop is None:
            # no hay relacional => evaluar como expr != 0 (convención mínima)
            t, _ = self._gen_expr(rel_tokens, 0)
            self.emit("!=", t, "0")
            return t, None

        left = rel_tokens[:j]
        right = rel_tokens[j+1:]

        tleft, _ = self._gen_expr(left, 0)
        # right puede ser expr
        tright, _ = self._gen_expr(right, 0)
        self.emit(relop, tleft, tright)  # p.ej. "<, T1, T2"
        return tleft, None

    # -------------- condición con cortocircuito y saltos TRUE/FALSE numéricos --------------
    # cond_tokens admite:  (rel)  |  (rel) || (rel)  |  (rel) && (rel)
    # Retorna: lista de índices a parchar a fin con N_end (para FALSE final)
    def gen_condition_do_while(self, cond_tokens, N_body):
        # tokenizar por '||' o '&&' (al nivel superior)
        # asumimos formato simple como acordado
        logic = None
        split_at = -1
        depth = 0
        for idx, tk in enumerate(cond_tokens):
            if tk["lex"] == "(": depth += 1
            elif tk["lex"] == ")": depth -= 1
            elif depth == 0 and tk["lex"] in ("||","&&"):
                logic = tk["lex"]; split_at = idx; break

        if logic is None:
            # solo relacional
            t, _s = self.gen_rel(cond_tokens)
            # TRUE -> body ; FALSE -> end (pendiente)
            n_true = self.emit("TRUE", str(N_body), "")
            n_false = self.emit("FALSE", "PENDING_END", "")
            return [n_false]
        else:
            left = cond_tokens[:split_at]
            right = cond_tokens[split_at+1:]

            if logic == "||":
                # A OR B:
                # eval A
                tA, _ = self.gen_rel(left)
                # TRUE -> body ; FALSE -> (inicio de B)
                n_true1 = self.emit("TRUE", str(N_body), "")
                # reservar salto a inicio de B con el siguiente N+1 (se conoce ahora)
                start_B = self.N + 1
                n_false1 = self.emit("FALSE", str(start_B), "")

                # eval B
                tB, _ = self.gen_rel(right)
                # TRUE -> body ; FALSE -> END (pendiente)
                n_true2 = self.emit("TRUE", str(N_body), "")
                n_false2 = self.emit("FALSE", "PENDING_END", "")
                return [n_false2]

            else:
                # A AND B (sin '!'):
                # eval A
                tA, _ = self.gen_rel(left)
                # Si A es FALSE -> END (pendiente)
                n_false1 = self.emit("FALSE", "PENDING_END", "")
                # Si A es TRUE -> evaluar B (siguiente N ya es inicio de B)
                # (no necesitamos fila TRUE aquí)
                # eval B
                tB, _ = self.gen_rel(right)
                # TRUE -> BODY ; FALSE -> END (pendiente)
                n_true2  = self.emit("TRUE",  str(N_body), "")
                n_false2 = self.emit("FALSE", "PENDING_END", "")
                return [n_false1, n_false2]

def _slice_until(tokens, i, stop_lexes):
    j = i
    while j < len(tokens) and tokens[j]["lex"] not in stop_lexes:
        j += 1
    return tokens[i:j], j

def _strip_parens(tokens):
    if len(tokens) >= 2 and tokens[0]["lex"] == "(" and tokens[-1]["lex"] == ")":
        # intentar quitar un par externo
        depth = 0
        ok = True
        for k, tk in enumerate(tokens):
            if tk["lex"] == "(": depth += 1
            elif tk["lex"] == ")":
                depth -= 1
                if depth == 0 and k != len(tokens)-1:
                    ok = False; break
        if ok:
            return tokens[1:-1]
    return tokens

def generar_triplos(texto):
    toks = lex(texto)
    em = TriploEmitter()
    i, n = 0, len(toks)

    # Para backpatch de múltiples do-while anidados
    end_patch_stacks = []

    while i < n:
        tk = toks[i]

        # --- do { ... } while (cond) ; ---
        if tk["tipo"] == "KW" and tk["lex"] == "do":
            i += 1
            # debe venir '{'
            if i < n and toks[i]["lex"] == "{":
                i += 1
                # cuerpo: recordamos el N inicial del cuerpo
                body_start_N = em.N + 1  # próxima fila será la primera del cuerpo

                # consumir sentencias hasta '}'
                while i < n and not (toks[i]["lex"] == "}"):
                    # Asignaciones dentro del cuerpo
                    if i+1 < n and toks[i]["tipo"] == "IDENT" and toks[i+1]["lex"] == "=":
                        lhs = toks[i]["lex"]; i += 2
                        rhs, j = _slice_until(toks, i, {";", "}"})
                        em.gen_assignment(lhs, rhs)
                        i = j
                        if i < n and toks[i]["lex"] == ";": i += 1
                    else:
                        i += 1  # ignorar cualquier cosa no soportada

                # cerrar cuerpo con '}'
                if i < n and toks[i]["lex"] == "}": i += 1

                # esperar 'while (cond) ;'
                if i < n and toks[i]["tipo"] == "KW" and toks[i]["lex"] == "while":
                    i += 1
                    if i < n and toks[i]["lex"] == "(":
                        # tomar la condición completa hasta ')'
                        j = i + 1; depth = 1
                        while j < n and depth > 0:
                            if toks[j]["lex"] == "(": depth += 1
                            elif toks[j]["lex"] == ")": depth -= 1
                            j += 1
                        cond_tokens = toks[i+1:j-1]  # entre paréntesis
                        cond_tokens = _strip_parens(cond_tokens)

                        # generar saltos TRUE/FALSE con num de fila
                        patches = em.gen_condition_do_while(cond_tokens, body_start_N)
                        end_patch_stacks.append((patches, len(em.rows)))  # recordar hasta dónde

                        # consumir ');'
                        i = j
                        if i < n and toks[i]["lex"] == ";": i += 1
                continue

        # --- Asignación simple fuera de do-while ---
        if i+1 < n and tk["tipo"] == "IDENT" and toks[i+1]["lex"] == "=":
            lhs = toks[i]["lex"]; i += 2
            rhs, j = _slice_until(toks, i, {";", "}"})
            em.gen_assignment(lhs, rhs)
            i = j
            if i < n and toks[i]["lex"] == ";": i += 1
            continue

        i += 1

    # Backpatch global: todas las filas con "PENDING_END" apuntan a la primera
    # fila después del último triplo del bucle correspondiente (N+1 al cierre).
    # Como usamos filas planas, el "end" natural es N+1 del momento en que termina el bucle.
    # Aquí, tras haber generado todo, el destino final de salida es em.N + 1
    finalNplus1 = em.N + 1
    for row in em.rows:
        if row["O"] in ("TRUE","FALSE") and row["DO"] == "PENDING_END":
            row["DO"] = str(finalNplus1)

    # Devolver como lista de tuplas para CSV: (N,O,DO,DF)
    triplos = [(r["N"], r["O"], r["DO"], r["DF"]) for r in em.rows]
    return triplos


# =========================================================================
#            API combinada para la GUI: símbolos, errores y triplos
# =========================================================================

def analizar_y_triplos(texto):
    tabla, errores = analizar_dos_pasadas(texto)
    triplos = generar_triplos(texto)
    return tabla, errores, triplos
